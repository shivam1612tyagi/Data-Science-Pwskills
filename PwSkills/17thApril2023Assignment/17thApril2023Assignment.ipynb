{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050525c0",
   "metadata": {},
   "source": [
    "# 17th April,2023 Assignment --------------- Shivam Tyagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8212cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans: Gradient Boosting Regression is a machine learning technique that combines multiple weak learners, typically decision trees, to create a strong predictive model.\n",
    "\n",
    "The algorithm works by iteratively adding weak learners to the model, with each learner trying to improve upon the mistakes of the previous learner.\n",
    "\n",
    "In Gradient Boosting Regression, the algorithm focuses on minimizing the residual error between the actual and predicted values, using gradient descent to find the optimal values for the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09d649",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417d659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bffad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3bd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [1, 2, 3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e8afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 1, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(gb, param_grid, cv=5)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "763644aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.2, max_depth=1, n_estimators=200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gb = GradientBoostingRegressor(**grid_search.best_params_)\n",
    "\n",
    "best_gb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20ecf814",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_gb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9f3ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 95.43345014876512 \n",
      "\n",
      "R-squared: 0.9946276779735156\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y, predictions)\n",
    "r2 = r2_score(y, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse, '\\n')\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7649406",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9149cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.1, 'max_depth': 8, 'min_samples_leaf': 3, 'min_samples_split': 8, 'n_estimators': 61, 'subsample': 0.5} \n",
      " \n",
      "\n",
      "Best score:  2435.058073479059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': sp_randint(10, 100),\n",
    "    'max_depth': sp_randint(2, 10),\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'min_samples_split': sp_randint(2, 20),\n",
    "    'min_samples_leaf': sp_randint(1, 10)\n",
    "}\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters: \", random_search.best_params_, '\\n \\n')\n",
    "print(\"Best score: \", -random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans: A weak learner in Gradient Boosting is a simple or relatively simple model that performs only slightly better than random guessing. In the context of Gradient Boosting Regression, a weak learner is typically a decision tree with a small number of splits or levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b43b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans:\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively add simple models to the ensemble, each model trying to correct the mistakes of the previous model.\n",
    "\n",
    "The algorithm starts with a simple model, such as a decision tree, and then adds more models to the ensemble in a way that minimizes the residual error between the actual and predicted values.\n",
    "\n",
    "At each iteration, the algorithm calculates the gradient of the loss function with respect to the predicted values, and then fits a weak learner to the negative gradient, essentially trying to predict the residual error of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans: The Gradient Boosting algorithm builds an ensemble of weak learners by adding them to the model sequentially, with each new learner trying to improve upon the mistakes of the previous learner.\n",
    "\n",
    "The algorithm starts by fitting a simple model, such as a decision tree, to the data.\n",
    "\n",
    "Then, it calculates the residuals between the actual and predicted values of the first model and fits a new model to the residuals.\n",
    "\n",
    "This process is repeated for a predetermined number of iterations or until a stopping criterion is met.\n",
    "\n",
    "The final prediction is the weighted sum of the predictions of all the weak learners in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Ans: The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1.Initialize the model with a simple model, such as a decision tree.\n",
    "2.Calculate the residuals between the actual and predicted values of the model.\n",
    "3.Fit a new model to the residuals, essentially trying to predict the errors of the previous model.\n",
    "4.Combine the predictions of the new model with the predictions of the previous model to form the updated predictions.\n",
    "5.Repeat steps 2-4 until a predetermined number of iterations or until a stopping criterion is met.\n",
    "6.Calculate the final prediction as the weighted sum of the predictions of all the models in the ensemble. The weights are determined by the performance of each model, with better-performing models receiving higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01936030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
